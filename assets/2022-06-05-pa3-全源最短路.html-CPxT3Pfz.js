import{_ as t,r as c,o as d,c as a,a as e,b as n,d as r,e as o}from"./app-CoV9NwP4.js";const l={},s=e("h2",{id:"environment",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#environment"},[e("span",null,"Environment")])],-1),p=e("p",null,"P100 GPU 最大支持每个 SM 64KB shared memory，但每个 thread block 最多只支持 48KB",-1),_=e("h2",{id:"method",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#method"},[e("span",null,"Method")])],-1),h={href:"https://lab.cs.tsinghua.edu.cn/hpc/doc/exp/3.apsp/",target:"_blank",rel:"noopener noreferrer"},m=e("code",null,"threadIdx",-1),b=e("code",null,"i_start",-1),f=e("code",null,"j_start",-1),v=e("code",null,"center_block_start",-1),u=o(`<h3 id="phase-1" tabindex="-1"><a class="header-anchor" href="#phase-1"><span>Phase 1</span></a></h3><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>k = [ p * b, (p + 1) * b )
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>对于每个 thread block, 访问范围包括 <code>k * k</code> 共 <code>b * b</code> 个 <code>int</code>, 也即需要 <code>b * b * sizeof(int)</code> 大小的 shared memory.</p><p>对于每个 <code>p</code>, 仅需一个 thread block 即可完成任务. 但是很浪费.</p><h3 id="phase-2" tabindex="-1"><a class="header-anchor" href="#phase-2"><span>Phase 2</span></a></h3><h4 id="horizontal" tabindex="-1"><a class="header-anchor" href="#horizontal"><span>Horizontal</span></a></h4><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>k = [ p * b          , (p + 1) * b          )
i = [ p * b          , (p + 1) * b          )
j = [ blockIdx.x * b , (blockIdx.x + 1) * b )
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>特别的, 若 <code>j</code> 的范围恰好在 center block 后, 即 <code>blockIdx.x * b &gt;= center_block_start</code> 时, 则需额外偏移 <code>b</code>.</p><p>对于每个 thread block, 访问范围包括 <code>i * j</code>, <code>i * k</code>, <code>k * j</code>, 其中 <code>i * j</code> 和 <code>k * j</code> 重合, 因此共 <code>2 * b * b</code> 个 <code>int</code>.</p><p>共需 <code>(ceil(n / p) - 1) * 1</code> 个 thread block.</p><h4 id="vertical" tabindex="-1"><a class="header-anchor" href="#vertical"><span>Vertical</span></a></h4><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>k = [ p * b          , (p + 1) * b          )
i = [ blockIdx.y * b , (blockIdx.y + 1) * b )
j = [ p * b          , (p + 1) * b          )
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>特别的, 若 <code>i</code> 的范围恰好在 center block 后, 即 <code>blockIdx.y * b &gt;= center_block_start</code> 时, 则需额外偏移 <code>b</code>.</p><p>对于每个 thread block, 访问范围包括 <code>i x j</code>, <code>i x k</code>, <code>k x j</code>, 其中 <code>i x j</code> 和 <code>i x k</code> 重合, 因此共 <code>2 * b * b</code> 个 <code>int</code>.</p><p>共需 <code>1 * (ceil(n / p) - 1)</code> 个 thread block.</p><h3 id="phase-3" tabindex="-1"><a class="header-anchor" href="#phase-3"><span>Phase 3</span></a></h3><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>k = [ p * b          , (p + 1) * b          )
i = [ blockIdx.y * b , (blockIdx.y + 1) * b )
i = [ blockIdx.x * b , (blockIdx.x + 1) * b )
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>特别的, 若 <code>i</code> 或 <code>j</code> 的范围恰好在 center block 后, 即 <code>blockIdx * b &gt;= center_block_start</code> 时, 则需额外偏移 <code>b</code>.</p><p>对于每个 thread block, 访问范围包括 <code>i x j</code>, <code>i x k</code>, <code>k x j</code>, 均不重合, 共 <code>3 * b * b</code> 个 <code>int</code>.</p><p>共需 <code>(ceil(n / p) - 1) * (ceil(n / p) - 1)</code> 个 thread block.</p><p>综合考虑, 取 <code>b = 32</code>, 每个 thread block 共 <code>32 x 32</code> 个 thread, 既不会超出 shared memory 限制, 又能够避免 bank conflict.</p><h2 id="performance" tabindex="-1"><a class="header-anchor" href="#performance"><span>Performance</span></a></h2><table><thead><tr><th>n</th><th><code>apspRef()</code> (ms)</th><th><code>apsp()</code> (ms)</th><th>Speedup</th></tr></thead><tbody><tr><td>1000</td><td>14.814903</td><td>2.969371</td><td>4.98923947</td></tr><tr><td>2500</td><td>377.148402</td><td>37.660415</td><td>10.01445157</td></tr><tr><td>5000</td><td>2972.073596</td><td>260.960028</td><td>11.38899938</td></tr><tr><td>7500</td><td>10016.146987</td><td>872.866804</td><td>11.47500047</td></tr><tr><td>10000</td><td>22632.211686</td><td>2060.573817</td><td>10.98345107</td></tr></tbody></table><p>在 <code>n = 1000</code> 下进行 profiling.</p><h3 id="nvprof-events" tabindex="-1"><a class="header-anchor" href="#nvprof-events"><span><code>nvprof</code> Events</span></a></h3><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>Invocations                                Event Name         Min         Max         Avg       Total
Device &quot;Tesla P100-PCIE-16GB (0)&quot;
    Kernel: _GLOBAL__N__51_tmpxft_000981f1_00000000_20_apsp_compute_61_cpp1_ii_034c69fe::Phase2KernelHorizontal(int, int*, int, int)
         96                   shared_ld_bank_conflict           0           0           0           0
         96                   shared_st_bank_conflict           0           0           0           0
    Kernel: _GLOBAL__N__51_tmpxft_000981f1_00000000_20_apsp_compute_61_cpp1_ii_034c69fe::Phase2KernelVertical(int, int*, int, int)
         96                   shared_ld_bank_conflict           0           0           0           0
         96                   shared_st_bank_conflict           0           0           0           0
    Kernel: _GLOBAL__N__51_tmpxft_000981f1_00000000_20_apsp_compute_61_cpp1_ii_034c69fe::Phase1Kernel(int, int*, int, int)
         96                   shared_ld_bank_conflict           0           0           0           0
         96                   shared_st_bank_conflict           0           0           0           0
    Kernel: _GLOBAL__N__51_tmpxft_000981f1_00000000_20_apsp_compute_61_cpp1_ii_034c69fe::Phase3Kernel(int, int*, int, int)
         96                   shared_ld_bank_conflict           0           0           0           0
         96                   shared_st_bank_conflict           0           0           0           0
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>没有出现 bank conflict.</p><h3 id="nvprof-metrics" tabindex="-1"><a class="header-anchor" href="#nvprof-metrics"><span><code>nvprof</code> Metrics</span></a></h3><div class="language-text line-numbers-mode" data-ext="text" data-title="text"><pre class="language-text"><code>Invocations                               Metric Name                         Metric Description         Min         Max         Avg
Device &quot;Tesla P100-PCIE-16GB (0)&quot;
    Kernel: _GLOBAL__N__51_tmpxft_000981f1_00000000_20_apsp_compute_61_cpp1_ii_034c69fe::Phase2KernelHorizontal(int, int*, int, int)
         96                         branch_efficiency                          Branch Efficiency     100.00%     100.00%     100.00%
         96                 warp_execution_efficiency                  Warp Execution Efficiency      97.97%     100.00%      98.03%
         96         warp_nonpred_execution_efficiency   Warp Non-Predicated Execution Efficiency      81.95%      95.89%      95.26%
         96                            gld_efficiency              Global Memory Load Efficiency     100.00%     100.00%     100.00%
         96                            gst_efficiency             Global Memory Store Efficiency     100.00%     100.00%     100.00%
         96                         shared_efficiency                   Shared Memory Efficiency      67.38%      69.64%      67.45%
    Kernel: _GLOBAL__N__51_tmpxft_000981f1_00000000_20_apsp_compute_61_cpp1_ii_034c69fe::Phase2KernelVertical(int, int*, int, int)
         96                         branch_efficiency                          Branch Efficiency     100.00%     100.00%     100.00%
         96                 warp_execution_efficiency                  Warp Execution Efficiency      51.59%     100.00%      98.49%
         96         warp_nonpred_execution_efficiency   Warp Non-Predicated Execution Efficiency      41.59%      97.89%      95.93%
         96                            gld_efficiency              Global Memory Load Efficiency     100.00%     100.00%     100.00%
         96                            gst_efficiency             Global Memory Store Efficiency     100.00%     100.00%     100.00%
         96                         shared_efficiency                   Shared Memory Efficiency      18.58%      69.01%      67.43%
    Kernel: _GLOBAL__N__51_tmpxft_000981f1_00000000_20_apsp_compute_61_cpp1_ii_034c69fe::Phase1Kernel(int, int*, int, int)
         96                         branch_efficiency                          Branch Efficiency     100.00%     100.00%     100.00%
         96                 warp_execution_efficiency                  Warp Execution Efficiency      47.13%     100.00%      98.35%
         96         warp_nonpred_execution_efficiency   Warp Non-Predicated Execution Efficiency      46.31%      97.76%      96.15%
         96                            gld_efficiency              Global Memory Load Efficiency     100.00%     100.00%     100.00%
         96                            gst_efficiency             Global Memory Store Efficiency     100.00%     100.00%     100.00%
         96                         shared_efficiency                   Shared Memory Efficiency      18.52%      68.69%      67.12%
    Kernel: _GLOBAL__N__51_tmpxft_000981f1_00000000_20_apsp_compute_61_cpp1_ii_034c69fe::Phase3Kernel(int, int*, int, int)
         96                         branch_efficiency                          Branch Efficiency     100.00%     100.00%     100.00%
         96                 warp_execution_efficiency                  Warp Execution Efficiency      98.12%     100.00%      98.18%
         96         warp_nonpred_execution_efficiency   Warp Non-Predicated Execution Efficiency      84.89%      95.93%      95.23%
         96                            gld_efficiency              Global Memory Load Efficiency     100.00%     100.00%     100.00%
         96                            gst_efficiency             Global Memory Store Efficiency     100.00%     100.00%     100.00%
         96                         shared_efficiency                   Shared Memory Efficiency      67.69%      69.91%      67.75%
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>可以看出各项指标的利用率都较充分, 但 shared memory 利用率较低.</p>`,30);function x(y,k){const i=c("ExternalLinkIcon");return d(),a("div",null,[s,p,_,e("p",null,[n("使用 "),e("a",h,[n("实验三 - 高性能计算导论实验文档 (tsinghua.edu.cn)"),r(i)]),n(" 中的分块方法. 一个 thread block 处理一个矩阵分块. 每个 thread block 所需使用的数据全部拷贝到 shared memory 中. 在 "),m,n(" 的基础上偏移 "),b,n(", "),f,n(" 或 "),v,n(" 即可将 shared memory 中的坐标映射到 global memory 中的不同矩阵分块.")]),u])}const E=t(l,[["render",x],["__file","2022-06-05-pa3-全源最短路.html.vue"]]),P=JSON.parse('{"path":"/2022/course-work/hpc/2022-06-05-pa3-%E5%85%A8%E6%BA%90%E6%9C%80%E7%9F%AD%E8%B7%AF.html","title":"PA3: 全源最短路","lang":"en-US","frontmatter":{"date":"2022-06-05T00:00:00.000Z","isOriginal":true,"category":["Course Work"],"tag":["CUDA","Introduction to High Performance Computing"],"title":"PA3: 全源最短路","description":"Environment P100 GPU 最大支持每个 SM 64KB shared memory，但每个 thread block 最多只支持 48KB Method 使用 实验三 - 高性能计算导论实验文档 (tsinghua.edu.cn) 中的分块方法. 一个 thread block 处理一个矩阵分块. 每个 thread block 所需使...","head":[["meta",{"property":"og:url","content":"https://blog.liblaf.me/2022/course-work/hpc/2022-06-05-pa3-%E5%85%A8%E6%BA%90%E6%9C%80%E7%9F%AD%E8%B7%AF.html"}],["meta",{"property":"og:site_name","content":"Blog"}],["meta",{"property":"og:title","content":"PA3: 全源最短路"}],["meta",{"property":"og:description","content":"Environment P100 GPU 最大支持每个 SM 64KB shared memory，但每个 thread block 最多只支持 48KB Method 使用 实验三 - 高性能计算导论实验文档 (tsinghua.edu.cn) 中的分块方法. 一个 thread block 处理一个矩阵分块. 每个 thread block 所需使..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2024-03-03T09:57:39.000Z"}],["meta",{"property":"article:author","content":"liblaf"}],["meta",{"property":"article:tag","content":"CUDA"}],["meta",{"property":"article:tag","content":"Introduction to High Performance Computing"}],["meta",{"property":"article:published_time","content":"2022-06-05T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2024-03-03T09:57:39.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"PA3: 全源最短路\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2022-06-05T00:00:00.000Z\\",\\"dateModified\\":\\"2024-03-03T09:57:39.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"liblaf\\",\\"url\\":\\"https://liblaf.me\\",\\"email\\":\\"i@liblaf.me\\"}]}"]]},"headers":[{"level":2,"title":"Environment","slug":"environment","link":"#environment","children":[]},{"level":2,"title":"Method","slug":"method","link":"#method","children":[{"level":3,"title":"Phase 1","slug":"phase-1","link":"#phase-1","children":[]},{"level":3,"title":"Phase 2","slug":"phase-2","link":"#phase-2","children":[]},{"level":3,"title":"Phase 3","slug":"phase-3","link":"#phase-3","children":[]}]},{"level":2,"title":"Performance","slug":"performance","link":"#performance","children":[{"level":3,"title":"nvprof Events","slug":"nvprof-events","link":"#nvprof-events","children":[]},{"level":3,"title":"nvprof Metrics","slug":"nvprof-metrics","link":"#nvprof-metrics","children":[]}]}],"git":{"createdTime":1677217008000,"updatedTime":1709459859000,"contributors":[{"name":"liblaf","email":"30631553+liblaf@users.noreply.github.com","commits":3}]},"readingTime":{"minutes":3.09,"words":926},"filePathRelative":"2022/course-work/hpc/2022-06-05-pa3-全源最短路.md","localizedDate":"June 5, 2022","excerpt":"<h2>Environment</h2>\\n<p>P100 GPU 最大支持每个 SM 64KB shared memory，但每个 thread block 最多只支持 48KB</p>\\n<h2>Method</h2>\\n<p>使用 <a href=\\"https://lab.cs.tsinghua.edu.cn/hpc/doc/exp/3.apsp/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">实验三 - 高性能计算导论实验文档 (tsinghua.edu.cn)</a> 中的分块方法. 一个 thread block 处理一个矩阵分块. 每个 thread block 所需使用的数据全部拷贝到 shared memory 中. 在 <code>threadIdx</code> 的基础上偏移 <code>i_start</code>, <code>j_start</code> 或 <code>center_block_start</code> 即可将 shared memory 中的坐标映射到 global memory 中的不同矩阵分块.</p>","autoDesc":true}');export{E as comp,P as data};
