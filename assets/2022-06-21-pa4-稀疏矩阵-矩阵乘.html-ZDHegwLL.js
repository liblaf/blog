import{_ as t,o as e,c as d,e as r}from"./app-CoV9NwP4.js";const a={},n=r('<h2 id="优化策略" tabindex="-1"><a class="header-anchor" href="#优化策略"><span>优化策略</span></a></h2><h3 id="warp-divergence" tabindex="-1"><a class="header-anchor" href="#warp-divergence"><span>warp divergence</span></a></h3><p>ref 实现中的 warp divergence 主要是因为将不同行归入一个 warp 计算, 而不同行的 NNZ 可能有很大差异, 产生 warp divergence. 因此, 只要避免将不同行划入同一 warp 即可. 因此, 令 <code>block.x = 1</code>, 使每个 thread block 至多处理一行数据.</p><p>如图中 ref 与 phase_1 对比, 提升有限.</p><h3 id="shared-memory" tabindex="-1"><a class="header-anchor" href="#shared-memory"><span>shared memory</span></a></h3><p>在 SpMM 中, 稀疏矩阵的一个元素代表了对于稠密矩阵的一行的访问. 因此可以将稀疏矩阵的一部分缓存在 shared memory 中, 以减少重复从 global memory 中读取稀疏矩阵.</p><p>如图中 phase_2, 效果拔群.</p><h3 id="load-imbalance" tabindex="-1"><a class="header-anchor" href="#load-imbalance"><span>load imbalance</span></a></h3><p>稀疏矩阵不同行的 NNZ 可能有很大差异, 因此考虑将较大的行进一步划分, 将稀疏矩阵的一行分割为多个 <code>Task</code>, 分配到多个线程中处理, 再使用 <code>atomicAdd</code> 归约. 为了减少同一行被连续线程处理导致 atomic 冲突频繁, 我们可以将 <code>Task</code> 打乱.</p><p>如图中 opt, 获得了进一步提升.</p><figure><img src="https://cdn.liblaf.me/img/2023/2023-02-24T051015Z.png" alt="opt" tabindex="0" loading="lazy"><figcaption>opt</figcaption></figure><h2 id="performance" tabindex="-1"><a class="header-anchor" href="#performance"><span>Performance</span></a></h2><figure><img src="https://cdn.liblaf.me/img/2023/2023-02-24T051034Z.png" alt="speedup" tabindex="0" loading="lazy"><figcaption>speedup</figcaption></figure><h3 id="klen-32" tabindex="-1"><a class="header-anchor" href="#klen-32"><span><code>kLen = 32</code></span></a></h3><table><thead><tr><th>Dataset</th><th>ref time</th><th>opt time</th><th>speedup</th></tr></thead><tbody><tr><td>arxiv</td><td>0.000772655</td><td>0.000382011</td><td>2.0225988256882657</td></tr><tr><td>collab</td><td>0.00133105</td><td>0.000747854</td><td>1.7798260088199034</td></tr><tr><td>citation</td><td>0.0164488</td><td>0.0109764</td><td>1.4985605480849822</td></tr><tr><td>ddi</td><td>0.000705377</td><td>0.000233554</td><td>3.0201880507291676</td></tr><tr><td>protein</td><td>0.0246445</td><td>0.0136007</td><td>1.8120023234098244</td></tr><tr><td>ppa</td><td>0.0183861</td><td>0.0103118</td><td>1.783015574390504</td></tr><tr><td>reddit.dgl</td><td>0.0486643</td><td>0.0211276</td><td>2.303352013479998</td></tr><tr><td>products</td><td>0.0558278</td><td>0.0334461</td><td>1.669187139905699</td></tr><tr><td>youtube</td><td>0.00364472</td><td>0.00274129</td><td>1.3295638184942127</td></tr><tr><td>amazon_cogdl</td><td>0.125264</td><td>0.0522025</td><td>2.399578564245007</td></tr><tr><td>yelp</td><td>0.00658028</td><td>0.00378241</td><td>1.7397056374110687</td></tr><tr><td>wikikg2</td><td>0.00714436</td><td>0.00485393</td><td>1.4718712465981172</td></tr><tr><td>am</td><td>0.00374</td><td>0.00226176</td><td>1.6535795132993776</td></tr></tbody></table><h3 id="klen-256" tabindex="-1"><a class="header-anchor" href="#klen-256"><span><code>kLen = 256</code></span></a></h3><table><thead><tr><th>Dataset</th><th>ref time</th><th>opt time</th><th>speedup</th></tr></thead><tbody><tr><td>arxiv</td><td>0.0030007</td><td>0.00297216</td><td>1.0096024440137812</td></tr><tr><td>collab</td><td>0.00520226</td><td>0.00587065</td><td>0.8861471898341752</td></tr><tr><td>citation</td><td>0.0788706</td><td>0.119014</td><td>0.6627001865326768</td></tr><tr><td>ddi</td><td>0.00156062</td><td>0.00159544</td><td>0.978175299603871</td></tr><tr><td>protein</td><td>0.0808061</td><td>0.112618</td><td>0.7175238416594151</td></tr><tr><td>ppa</td><td>0.0849794</td><td>0.0839621</td><td>1.0121161809911854</td></tr><tr><td>reddit.dgl</td><td>0.202341</td><td>0.174222</td><td>1.1613975272927644</td></tr><tr><td>products</td><td>0.258469</td><td>0.274967</td><td>0.9400000727360011</td></tr><tr><td>youtube</td><td>0.0144181</td><td>0.0220068</td><td>0.6551656760637621</td></tr><tr><td>amazon_cogdl</td><td>0.517086</td><td>0.424054</td><td>1.2193871535229006</td></tr><tr><td>yelp</td><td>0.029976</td><td>0.0305569</td><td>0.980989563731923</td></tr><tr><td>wikikg2</td><td>0.0166503</td><td>0.0387169</td><td>0.4300525093692935</td></tr><tr><td>am</td><td>0.0134264</td><td>0.0182773</td><td>0.7345942781483041</td></tr></tbody></table>',17),o=[n];function i(l,p){return e(),d("div",null,o)}const h=t(a,[["render",i],["__file","2022-06-21-pa4-稀疏矩阵-矩阵乘.html.vue"]]),m=JSON.parse('{"path":"/2022/course-work/hpc/2022-06-21-pa4-%E7%A8%80%E7%96%8F%E7%9F%A9%E9%98%B5-%E7%9F%A9%E9%98%B5%E4%B9%98.html","title":"PA4: 稀疏矩阵-矩阵乘","lang":"en-US","frontmatter":{"date":"2022-06-21T00:00:00.000Z","isOriginal":true,"category":["Course Work"],"tag":["CUDA","Introduction to High Performance Computing","SpMM"],"title":"PA4: 稀疏矩阵-矩阵乘","description":"优化策略 warp divergence ref 实现中的 warp divergence 主要是因为将不同行归入一个 warp 计算, 而不同行的 NNZ 可能有很大差异, 产生 warp divergence. 因此, 只要避免将不同行划入同一 warp 即可. 因此, 令 block.x = 1, 使每个 thread block 至多处理一行数...","head":[["meta",{"property":"og:url","content":"https://blog.liblaf.me/2022/course-work/hpc/2022-06-21-pa4-%E7%A8%80%E7%96%8F%E7%9F%A9%E9%98%B5-%E7%9F%A9%E9%98%B5%E4%B9%98.html"}],["meta",{"property":"og:site_name","content":"Blog"}],["meta",{"property":"og:title","content":"PA4: 稀疏矩阵-矩阵乘"}],["meta",{"property":"og:description","content":"优化策略 warp divergence ref 实现中的 warp divergence 主要是因为将不同行归入一个 warp 计算, 而不同行的 NNZ 可能有很大差异, 产生 warp divergence. 因此, 只要避免将不同行划入同一 warp 即可. 因此, 令 block.x = 1, 使每个 thread block 至多处理一行数..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://cdn.liblaf.me/img/2023/2023-02-24T051015Z.png"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:updated_time","content":"2024-03-03T09:57:39.000Z"}],["meta",{"property":"article:author","content":"liblaf"}],["meta",{"property":"article:tag","content":"CUDA"}],["meta",{"property":"article:tag","content":"Introduction to High Performance Computing"}],["meta",{"property":"article:tag","content":"SpMM"}],["meta",{"property":"article:published_time","content":"2022-06-21T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2024-03-03T09:57:39.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"PA4: 稀疏矩阵-矩阵乘\\",\\"image\\":[\\"https://cdn.liblaf.me/img/2023/2023-02-24T051015Z.png\\",\\"https://cdn.liblaf.me/img/2023/2023-02-24T051034Z.png\\"],\\"datePublished\\":\\"2022-06-21T00:00:00.000Z\\",\\"dateModified\\":\\"2024-03-03T09:57:39.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"liblaf\\",\\"url\\":\\"https://liblaf.me\\",\\"email\\":\\"i@liblaf.me\\"}]}"]]},"headers":[{"level":2,"title":"优化策略","slug":"优化策略","link":"#优化策略","children":[{"level":3,"title":"warp divergence","slug":"warp-divergence","link":"#warp-divergence","children":[]},{"level":3,"title":"shared memory","slug":"shared-memory","link":"#shared-memory","children":[]},{"level":3,"title":"load imbalance","slug":"load-imbalance","link":"#load-imbalance","children":[]}]},{"level":2,"title":"Performance","slug":"performance","link":"#performance","children":[{"level":3,"title":"kLen = 32","slug":"klen-32","link":"#klen-32","children":[]},{"level":3,"title":"kLen = 256","slug":"klen-256","link":"#klen-256","children":[]}]}],"git":{"createdTime":1677217008000,"updatedTime":1709459859000,"contributors":[{"name":"liblaf","email":"30631553+liblaf@users.noreply.github.com","commits":4}]},"readingTime":{"minutes":1.42,"words":427},"filePathRelative":"2022/course-work/hpc/2022-06-21-pa4-稀疏矩阵-矩阵乘.md","localizedDate":"June 21, 2022","excerpt":"<h2>优化策略</h2>\\n<h3>warp divergence</h3>\\n<p>ref 实现中的 warp divergence 主要是因为将不同行归入一个 warp 计算, 而不同行的 NNZ 可能有很大差异, 产生 warp divergence. 因此, 只要避免将不同行划入同一 warp 即可. 因此, 令 <code>block.x = 1</code>, 使每个 thread block 至多处理一行数据.</p>\\n<p>如图中 ref 与 phase_1 对比, 提升有限.</p>\\n<h3>shared memory</h3>\\n<p>在 SpMM 中, 稀疏矩阵的一个元素代表了对于稠密矩阵的一行的访问. 因此可以将稀疏矩阵的一部分缓存在 shared memory 中, 以减少重复从 global memory 中读取稀疏矩阵.</p>","autoDesc":true}');export{h as comp,m as data};
